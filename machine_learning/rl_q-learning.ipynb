{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增强学习：Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 游戏棋盘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们创造一个游戏棋盘（`gym`上有许多类似的游戏棋盘可以使用，这里我们自己实现一个简单的游戏棋盘）。  \n",
    "游戏棋盘上的元素主要包括:  \n",
    "- 0: 可以走的道路\n",
    "- X: 陷阱，走入陷阱则算游戏失败\n",
    "- P: 玩家当前状态(位置)\n",
    "- G: 终点，当走到终点时，则算游戏胜利\n",
    "\n",
    "这里，设定初始奖励值为0，每走一步奖励值减0.1，掉入陷阱则奖励值减10，走到终点奖励值加10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayEnvironment:\n",
    "    def __init__(self, table_size=(5, 5), traps=[5, 7, 11, 12], \n",
    "                 step_cost=-0.1, trap_cost=-10, goal_cost=10, start_state=0, goal_state=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param table_size: 棋盘大小 (length, width)\n",
    "        :param traps: 陷阱位置\n",
    "        :param step_cost: 移动损失\n",
    "        :param trap_cost: 进入陷阱损失\n",
    "        :param goal_cost: 到底终点奖励\n",
    "        :param start_state: 起始状态(位置)\n",
    "        :param gold_state: 终点状态(位置)\n",
    "        \"\"\"\n",
    "        self.table_size = table_size\n",
    "        self.length = table_size[0]\n",
    "        self.width = table_size[1]\n",
    "        \n",
    "        self.start_state = start_state\n",
    "        self.current_state = start_state\n",
    "        self.current_step = 0\n",
    "        # 如果未传入终点状态，则将最后个状态作为终点状态\n",
    "        if goal_state is None:\n",
    "            self.goal_state = self.length * self.width - 1\n",
    "        else:\n",
    "            self.goal_state = goal_state\n",
    "        self.traps = traps\n",
    "        \n",
    "        self.step_cost = step_cost\n",
    "        self.trap_cost = trap_cost\n",
    "        self.goal_cost = goal_cost\n",
    "        self.cumulative_reward = 0\n",
    "        \n",
    "        self.play_status = 'START'\n",
    "        \n",
    "    def reset_evn(self):\n",
    "        \"\"\"\n",
    "        重置游戏环境\n",
    "        \"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        self.current_step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.play_status = 'START'\n",
    "        return self.current_state\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return self.length * self.width\n",
    "    \n",
    "    def get_action_size(self):\n",
    "        return 4\n",
    "    \n",
    "    def get_random_walk(self):\n",
    "        return np.random.choice(np.arange(4))\n",
    "    \n",
    "    def render_chessboard(self, return_chessboard=False):\n",
    "        \"\"\"\n",
    "        游戏棋盘\n",
    "        X: 陷阱\n",
    "        P: 玩家当前位置\n",
    "        S: 起始位置\n",
    "        G: 终点位置\n",
    "        :param return_chessboard: 是否返回游戏棋盘，如果不返回，则打印游戏棋盘\n",
    "        \"\"\"\n",
    "        chessboard = np.full(self.length * self.width, '0')\n",
    "        chessboard[self.traps] = 'X'\n",
    "        chessboard[self.start_state] = 'S'\n",
    "        chessboard[self.goal_state] = 'G'\n",
    "        chessboard[self.current_state] = 'P'\n",
    "        \n",
    "        chessboard = chessboard.reshape(self.width, self.length)\n",
    "        if return_chessboard:\n",
    "            return chessboard\n",
    "        else:\n",
    "            print(\"=====step: {}=====\".format(self.current_step))\n",
    "            print(\"state: {}\\tstatus: {}\\treawrd: {}\".format(self.current_state, \n",
    "                                                              self.play_status, \n",
    "                                                              self.cumulative_reward))\n",
    "            for r in chessboard:\n",
    "                print(r)\n",
    "            \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param action:\n",
    "            0 - up\n",
    "            1 - right\n",
    "            2 - down\n",
    "            3 - left\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        reward = 0\n",
    "        if action == 0:\n",
    "            new_state = self.current_state - self.length\n",
    "            # 检测是否超出了上边界，如果超出了，则设置为原地\n",
    "            if new_state < 0:\n",
    "                new_state = self.current_state\n",
    "        if action == 1:\n",
    "            # 检测是否超出了右边界，如果超出了，则设置为原地\n",
    "            if self.current_state % self.length == self.length - 1:\n",
    "                new_state = self.current_state\n",
    "            else:\n",
    "                new_state = self.current_state + 1\n",
    "        if action == 2:\n",
    "            new_state = self.current_state + self.length\n",
    "            # 检测是否超出了下边界，如果超出了，则设置为原地\n",
    "            if new_state >= self.length * self.width:\n",
    "                new_state = self.current_state\n",
    "        if action == 3:\n",
    "            # 检测是否超出了左边界，如果超出了，则设置为原地\n",
    "            if self.current_state % self.length == 0:\n",
    "                new_state = self.current_state\n",
    "            else:\n",
    "                new_state = self.current_state - 1\n",
    "        \n",
    "        done, reward = self.cal_state_reward(new_state)\n",
    "        self.cumulative_reward += reward\n",
    "        self.current_state = new_state\n",
    "        self.current_step += 1\n",
    "        if done:\n",
    "            if new_state == self.goal_state:\n",
    "                self.play_status = 'GOAL'\n",
    "            else:\n",
    "                self.play_status = 'FAIL'\n",
    "        else:\n",
    "            self.play_status = 'PALYING'\n",
    "            \n",
    "        return done, reward, new_state\n",
    "    \n",
    "    def cal_state_reward(self, new_state):\n",
    "        \"\"\"\n",
    "        计算新状态得分\n",
    "        \"\"\"\n",
    "        if new_state in self.traps:\n",
    "            return True, self.trap_cost\n",
    "        if new_state == self.goal_state:\n",
    "            return True, self.goal_cost\n",
    "        return False, self.step_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建的游戏棋盘，设置陷阱位置，然后查看游戏棋盘的样子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====step: 0=====\n",
      "state: 0\tstatus: START\treawrd: 0\n",
      "['P' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n"
     ]
    }
   ],
   "source": [
    "play_env = PlayEnvironment(traps=[5, 6, 7, 14, 17, 18, 20])\n",
    "play_env.render_chessboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们往右移动一步，看下效果（`action`输入为1时，表示右移）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====step: 1=====\n",
      "state: 1\tstatus: PALYING\treawrd: -0.1\n",
      "['S' 'P' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n"
     ]
    }
   ],
   "source": [
    "play_env.step(1)\n",
    "play_env.render_chessboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了游戏棋盘后，可以开始实现Q-learning算法来玩游戏了。  \n",
    "Q-learning的详细介绍可以查看[这里](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)，下面将简单介绍一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，创建一个Q-table用来存储状态和行动的奖励。初始化Q-table后，按照下列公式对Q-table进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}  \n",
    "Q(s, a) = Q(s, a) + \\alpha(r + \\gamma maxQ(s', a') - Q(s, a))\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$Q$表示Q-function，用以计算输入状态和行动的奖励值，s为状态，s'为下一个状态，a为行动，a'为下一个行动，$\\alpha$为学习率，$\\gamma$为折扣率(discounting rate，用于衰减远期的奖励值影响)。  \n",
    "我们将采用[epsilon贪婪算法](https://imaddabbura.github.io/blog/data%20science/2018/03/31/epsilon-Greedy-Algorithm.html)来权衡探索(Exploration)与利用(Exploitation)。以$\\epsilon$的概率选择探索，如果进行探索，随机选择下一步的行动，如果进行利用，则利用现有的Q-table选择行动。  \n",
    "一开始将$\\epsilon$设置为1，主要以探索为主，随后逐渐衰减$\\epsilon$，增大利用的概率，按照下列公式进行$\\epsilon$更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}  \n",
    "\\epsilon = \\epsilon\\mathrm{e}^{-r}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中r为衰减率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先我们获取`state`和`action`的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_size = play_env.get_state_size()\n",
    "action_size = play_env.get_action_size()\n",
    "state_size, action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 迭代次数\n",
    "epochs = 5000\n",
    "# 学习率\n",
    "learning_rate = 0.75\n",
    "# 最大步数\n",
    "max_steps = 99\n",
    "# 折扣率\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "# epsilon衰减率\n",
    "decay_rate = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过Q-learning算法计算Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 统计游戏结果\n",
    "status_count = defaultdict(int)\n",
    "# 统计最终奖励\n",
    "final_rewards = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    state = play_env.reset_evn()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 确定这一步使用探索还是利用\n",
    "        ee_num = np.random.random(1)[0]\n",
    "        if ee_num > epsilon:\n",
    "            # 使用现有Q-table中最好的action\n",
    "            action = np.argmax(qtable[state, :])\n",
    "        else:\n",
    "            # 使用随机action\n",
    "            action = play_env.get_random_walk()\n",
    "            \n",
    "        done, reward, new_state = play_env.step(action)\n",
    "        # 按照Q-table更新公式更行qtable\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (\n",
    "            reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            status_count[play_env.play_status] += 1\n",
    "            final_rewards[epoch] = play_env.cumulative_reward\n",
    "            break\n",
    "    \n",
    "    if step == max_steps - 1:\n",
    "        status_count[play_env.play_statu] += 1\n",
    "        final_rewards[epoch] = play_env.cumulative_reward\n",
    "    # 更新epsilon\n",
    "    epsilon = epsilon * np.exp(-decay_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'FAIL': 394, 'GOAL': 4606})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练中最大的奖励值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rewards.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看Q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.48432105,   4.82560111, -10.        ,   4.4843204 ],\n",
       "       [  4.8256011 ,   5.18484327, -10.        ,   4.48432087],\n",
       "       [  5.18484327,   5.56299292, -10.        ,   4.82560111],\n",
       "       [  5.56299292,   5.18484251,   5.96104518,   5.18484228],\n",
       "       [  4.73914015,  -0.7199772 ,   3.9966546 ,   5.56299289],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  5.56298554,   5.5629929 ,   6.38004755, -10.        ],\n",
       "       [ -0.66926995,   3.78100487,  -9.99999762,   5.96104517],\n",
       "       [ -9.9609375 ,   7.28537125,  -0.38406818,  -0.37838299],\n",
       "       [-10.        ,   6.82110269,   7.774075  ,   6.82110257],\n",
       "       [-10.        ,   6.38004755,  -9.99999996,   7.28537125],\n",
       "       [  5.96104512, -10.        , -10.        ,   6.82110269],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  4.66263849,   7.77203361,  -9.99938965,  -0.32959487],\n",
       "       [  7.28512567,  -9.99999985,   8.2885    ,   7.25399627],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
       "       [  7.77407407,   8.83      ,   8.28849979,  -9.9999994 ],\n",
       "       [ -9.9999994 ,   9.4       ,   8.82999996,   8.28849858],\n",
       "       [ -9.99999762,  10.        ,   9.39999996,   8.82996632],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play game!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的Q-learning算法进行游戏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====step: 0=====\n",
      "state: 0\tstatus: START\treawrd: 0\n",
      "['P' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 1=====\n",
      "state: 1\tstatus: PALYING\treawrd: -0.1\n",
      "['S' 'P' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 2=====\n",
      "state: 2\tstatus: PALYING\treawrd: -0.2\n",
      "['S' '0' 'P' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 3=====\n",
      "state: 3\tstatus: PALYING\treawrd: -0.30000000000000004\n",
      "['S' '0' '0' 'P' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 4=====\n",
      "state: 8\tstatus: PALYING\treawrd: -0.4\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' 'P' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 5=====\n",
      "state: 13\tstatus: PALYING\treawrd: -0.5\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' 'P' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 6=====\n",
      "state: 12\tstatus: PALYING\treawrd: -0.6\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' 'P' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 7=====\n",
      "state: 11\tstatus: PALYING\treawrd: -0.7\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' 'P' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 8=====\n",
      "state: 16\tstatus: PALYING\treawrd: -0.7999999999999999\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' 'P' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'G']\n",
      "=====step: 9=====\n",
      "state: 21\tstatus: PALYING\treawrd: -0.8999999999999999\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' 'P' '0' '0' 'G']\n",
      "=====step: 10=====\n",
      "state: 22\tstatus: PALYING\treawrd: -0.9999999999999999\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' 'P' '0' 'G']\n",
      "=====step: 11=====\n",
      "state: 23\tstatus: PALYING\treawrd: -1.0999999999999999\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' 'P' 'G']\n",
      "=====step: 12=====\n",
      "state: 24\tstatus: GOAL\treawrd: 8.9\n",
      "['S' '0' '0' '0' '0']\n",
      "['X' 'X' 'X' '0' '0']\n",
      "['0' '0' '0' '0' 'X']\n",
      "['0' '0' 'X' 'X' '0']\n",
      "['X' '0' '0' '0' 'P']\n",
      "游戏结束，顺利到达终点！\n"
     ]
    }
   ],
   "source": [
    "state = play_env.reset_evn()\n",
    "play_env.render_chessboard()\n",
    "for step in range(max_steps):\n",
    "    action = np.argmax(qtable[state, :])\n",
    "    done, _, new_state = play_env.step(action)\n",
    "    play_env.render_chessboard()\n",
    "    if done:\n",
    "        if play_env.play_status == 'GOAL':\n",
    "            print('游戏结束，顺利到达终点！')\n",
    "        else:\n",
    "            print('游戏结束，失败！')\n",
    "        break\n",
    "    state = new_state\n",
    "else:\n",
    "    print('已经到达最大步数，游戏还在进行中，似乎迷路了...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
