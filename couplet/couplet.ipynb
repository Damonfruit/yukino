{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于Seq2Seq模型实现自动对下联"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要介绍了使用`keras`来构建一个Seq2Seq模型，实现自动对下联的功能    \n",
    "本文的对联数据可以在[这里](https://github.com/wb14123/couplet-dataset)找到，训练集中大约有70多万条对联，这里将前两个字符稍作修改，将其改成自定义的“结束”($)和“开始”(^)符号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读入字符集，一共大约有9000多个字符和汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['$', '^', '。', '，', '风', '春', '一', '人', '月', '山']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABS = []\n",
    "with open('couplet/vocabs', encoding='utf8') as f:\n",
    "    for vocab in f:\n",
    "        VOCABS.append(vocab.strip())\n",
    "\n",
    "VOCABS[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABS_SIZE = len(VOCABS)\n",
    "# 设置最长的句子为30个字符\n",
    "MAX_SENTENCE_LENGTH = 30\n",
    "START_SYM = '^'\n",
    "END_SYM = '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_TO_INDEX = {v: i for i, v in enumerate(VOCABS)}\n",
    "INDEX_TO_VOCAB = {i: v for v, i in VOCAB_TO_INDEX.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于读入所有对联会消耗大量内存，所以这里定义一个读取对联的生成器，而不是一次性读入所有的对联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件中的对联\n",
    "def auto_read_couplet(file, max_len=MAX_SENTENCE_LENGTH):\n",
    "    while True:\n",
    "        with open(file, encoding='utf8') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                words = line.strip().split(' ')\n",
    "                if len(words) > max_len:\n",
    "                    line = f.readline()\n",
    "                    continue\n",
    "                yield words\n",
    "                line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将句子用END_SYM填补到最大长度\n",
    "def pad_sentece(sentence, max_len=MAX_SENTENCE_LENGTH):\n",
    "    sent_length = len(sentence)\n",
    "    for _ in range(sent_length, max_len):\n",
    "        sentence.append(END_SYM)\n",
    "    return sentence\n",
    "\n",
    "# 将句子转为向量\n",
    "def sentence_to_vec(sentence, feature_n):\n",
    "    X = []\n",
    "    for word in sentence:\n",
    "        v = np.zeros(feature_n)\n",
    "        v[VOCAB_TO_INDEX[word]] = 1\n",
    "        X.append(v)\n",
    "    return np.array(X)\n",
    "\n",
    "# 将向量转为句子\n",
    "def vec_to_sentence(vector):\n",
    "    seq = ''\n",
    "    for vec in vector:\n",
    "        index = np.argmax(vec)\n",
    "        word = INDEX_TO_VOCAB[index]\n",
    "        seq = seq + word\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成对联数据，每次返回`batch_size`大小的数据  \n",
    "函数返回为两部分，第一部分为输入部分，包括`encoder`的输入，即上联和`decoder`输入，第二部分为输出，即下联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(batch_size=128, max_len=MAX_SENTENCE_LENGTH, feature_n=VOCABS_SIZE):\n",
    "    input_generator = auto_read_couplet('couplet/train/in.txt')\n",
    "    output_generator = auto_read_couplet('couplet/train/out.txt')\n",
    "    while True:\n",
    "        encode_inputs = []\n",
    "        decode_inputs = []\n",
    "        outputs = []\n",
    "        for _ in range(batch_size):\n",
    "            input_encode_couplet = next(input_generator)\n",
    "            input_encode_couplet = pad_sentece(input_encode_couplet)\n",
    "            input_encode_couplet_vec = sentence_to_vec(input_encode_couplet, feature_n)\n",
    "            encode_inputs.append(input_encode_couplet_vec)\n",
    "            \n",
    "            output_couplet = next(output_generator)\n",
    "            output_couplet = pad_sentece(output_couplet)\n",
    "            outputs_vec = sentence_to_vec(output_couplet, feature_n)\n",
    "            outputs.append(outputs_vec)\n",
    "            \n",
    "            input_decode_couplet = [START_SYM] + output_couplet[:-1]\n",
    "            input_decode_couplet_vec = sentence_to_vec(input_decode_couplet, feature_n)\n",
    "            decode_inputs.append(input_decode_couplet_vec)\n",
    "            \n",
    "        yield [np.array(encode_inputs), np.array(decode_inputs)], np.array(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里采用的是基于encoder-decoder框架的Seq2Seq模型，更多模型的细节可以在[基于Eecode-Decoder框架的Seq2Seq模型](https://github.com/snowhyzhang/yukino/blob/master/deep_learning/encoder-decoder_seq2seq_model.ipynb)上找到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Coding\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "class Seq2SeqModel:\n",
    "    def __init__(self, model_name, feature_dim, output_len, hidden_size=128, output_activation='softmax', \n",
    "                 optimizer='adam', loss='categorical_crossentropy', metrics=['acc']):\n",
    "        self.model_name = model_name\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_len = output_len\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_activation = output_activation\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        self.__build_model()\n",
    "        self.__compile_model()\n",
    "        \n",
    "    def __build_model(self):\n",
    "        # sequence to sequence model\n",
    "        encoder_inputs = layers.Input(shape=(None, self.feature_dim))\n",
    "        encoder = layers.LSTM(self.hidden_size, return_state=True)\n",
    "        _, state_h, state_c = encoder(encoder_inputs)\n",
    "        encoder_state = [state_h, state_c]\n",
    "\n",
    "        decoder_inputs = layers.Input(shape=(None, self.feature_dim))\n",
    "        decoder_lstm = layers.LSTM(self.hidden_size, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_state)\n",
    "        decoder_dense = layers.Dense(self.feature_dim, activation=self.output_activation)\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder_model = Model(encoder_inputs, encoder_state)\n",
    "        \n",
    "        # decoder\n",
    "        decoder_state_input_h = layers.Input(shape=(self.hidden_size, ))\n",
    "        decoder_state_input_c = layers.Input(shape=(self.hidden_size, ))\n",
    "        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "        decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, \n",
    "                                                         initial_state=decoder_states_inputs)\n",
    "        decoder_states = [state_h, state_c]\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "        self.decoder_model = Model([decoder_inputs] + decoder_states_inputs, \n",
    "                                   [decoder_outputs] + decoder_states)\n",
    "    def __compile_model(self):\n",
    "        self.model.compile(self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "    \n",
    "    def train(self, encode_inputs, decode_inputs, outputs, validation_data=None,\n",
    "              epochs=1, batch_size=128, verbose=1, callbacks=None):\n",
    "        self.model.fit([encode_inputs, decode_inputs], outputs, validation_data=validation_data,\n",
    "                       epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=callbacks)\n",
    "    \n",
    "    def train_generator(self, gen, validation_data=None, steps_per_epoch=128, epochs=1, \n",
    "                        verbose=1, callbacks=None):\n",
    "        self.model.fit_generator(gen, validation_data=validation_data, steps_per_epoch=steps_per_epoch, \n",
    "                                 epochs=epochs, verbose=verbose, callbacks=callbacks)\n",
    "    \n",
    "    def predict(self, seq):\n",
    "        state = self.encoder_model.predict(seq)\n",
    "        target_seq = np.array([0 for _ in range(self.feature_dim)]).reshape(1, 1, self.feature_dim)\n",
    "        output = []\n",
    "        for _ in range(self.output_len):\n",
    "            y_pred, h, c = self.decoder_model.predict([target_seq] + state)\n",
    "            output.append(y_pred[0, 0, :])\n",
    "            state = [h, c]\n",
    "            target_seq = y_pred\n",
    "        return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model = Seq2SeqModel('couplet', VOCABS_SIZE, MAX_SENTENCE_LENGTH, hidden_size=256)\n",
    "data_generator = generate_data(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_model.train_generator(data_generator, steps_per_epoch=128, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型来对下联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_couplet(couplet):\n",
    "    input_couplet = list(couplet)\n",
    "    input_couplet = pad_sentece(input_couplet)\n",
    "    input_couplet = sentence_to_vec(input_couplet, VOCABS_SIZE)\n",
    "    pred = seq2seq_model.predict(input_couplet[np.newaxis, ...])\n",
    "    output_couplet = vec_to_sentence(pred).replace(END_SYM, '')\n",
    "    \n",
    "    print(f'input : {couplet}\\noutput: {output_couplet}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : 秋雨无情，三杯浅醉\n",
      "output: 春风有梦，一点清香\n"
     ]
    }
   ],
   "source": [
    "predict_couplet('秋雨无情，三杯浅醉')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
